{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1331aef-efaf-4e95-9edb-869249f141a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] | D Loss: 1.2242 | G Loss: 0.7832\n",
      "Epoch [2/100] | D Loss: 2.2649 | G Loss: 0.6394\n",
      "Epoch [3/100] | D Loss: 0.5141 | G Loss: 1.4556\n",
      "Epoch [4/100] | D Loss: 0.8721 | G Loss: 1.0370\n",
      "Epoch [5/100] | D Loss: 1.1507 | G Loss: 0.9059\n",
      "Epoch [6/100] | D Loss: 1.1648 | G Loss: 1.0345\n",
      "Epoch [7/100] | D Loss: 1.6258 | G Loss: 0.8151\n",
      "Epoch [8/100] | D Loss: 0.9099 | G Loss: 1.0377\n",
      "Epoch [9/100] | D Loss: 1.1266 | G Loss: 1.0647\n",
      "Epoch [10/100] | D Loss: 1.0379 | G Loss: 1.1276\n",
      "Epoch [11/100] | D Loss: 1.2596 | G Loss: 0.8560\n",
      "Epoch [12/100] | D Loss: 1.0583 | G Loss: 0.9820\n",
      "Epoch [13/100] | D Loss: 1.3213 | G Loss: 0.8313\n",
      "Epoch [14/100] | D Loss: 1.2403 | G Loss: 0.8025\n",
      "Epoch [15/100] | D Loss: 1.2772 | G Loss: 0.7735\n",
      "Epoch [16/100] | D Loss: 1.2901 | G Loss: 0.8173\n",
      "Epoch [17/100] | D Loss: 1.3058 | G Loss: 0.7530\n",
      "Epoch [18/100] | D Loss: 1.2322 | G Loss: 0.7997\n",
      "Epoch [19/100] | D Loss: 1.2193 | G Loss: 0.7995\n",
      "Epoch [20/100] | D Loss: 1.0952 | G Loss: 0.9630\n",
      "Epoch [21/100] | D Loss: 1.1054 | G Loss: 1.0510\n",
      "Epoch [22/100] | D Loss: 0.8867 | G Loss: 1.1775\n",
      "Epoch [23/100] | D Loss: 0.9243 | G Loss: 1.2053\n",
      "Epoch [24/100] | D Loss: 1.6279 | G Loss: 1.5716\n",
      "Epoch [25/100] | D Loss: 0.6514 | G Loss: 1.6861\n",
      "Epoch [26/100] | D Loss: 0.8149 | G Loss: 1.6454\n",
      "Epoch [27/100] | D Loss: 1.1177 | G Loss: 0.9319\n",
      "Epoch [28/100] | D Loss: 1.1267 | G Loss: 0.9286\n",
      "Epoch [29/100] | D Loss: 1.2283 | G Loss: 0.8107\n",
      "Epoch [30/100] | D Loss: 1.0286 | G Loss: 1.0015\n",
      "Epoch [31/100] | D Loss: 1.2408 | G Loss: 0.7441\n",
      "Epoch [32/100] | D Loss: 1.2068 | G Loss: 0.8219\n",
      "Epoch [33/100] | D Loss: 1.0642 | G Loss: 1.0101\n",
      "Epoch [34/100] | D Loss: 1.2100 | G Loss: 1.0202\n",
      "Epoch [35/100] | D Loss: 1.3298 | G Loss: 0.7138\n",
      "Epoch [36/100] | D Loss: 1.1342 | G Loss: 1.0260\n",
      "Epoch [37/100] | D Loss: 1.2504 | G Loss: 0.9679\n",
      "Epoch [38/100] | D Loss: 1.0240 | G Loss: 1.0864\n",
      "Epoch [39/100] | D Loss: 1.2048 | G Loss: 0.8061\n",
      "Epoch [40/100] | D Loss: 1.1679 | G Loss: 0.8320\n",
      "Epoch [41/100] | D Loss: 1.1055 | G Loss: 1.1443\n",
      "Epoch [42/100] | D Loss: 1.1253 | G Loss: 0.8837\n",
      "Epoch [43/100] | D Loss: 1.1237 | G Loss: 1.0890\n",
      "Epoch [44/100] | D Loss: 1.3220 | G Loss: 0.8928\n",
      "Epoch [45/100] | D Loss: 1.1695 | G Loss: 0.9361\n",
      "Epoch [46/100] | D Loss: 1.0660 | G Loss: 1.0868\n",
      "Epoch [47/100] | D Loss: 1.1362 | G Loss: 1.0241\n",
      "Epoch [48/100] | D Loss: 1.1724 | G Loss: 0.8483\n",
      "Epoch [49/100] | D Loss: 1.2006 | G Loss: 0.8490\n",
      "Epoch [50/100] | D Loss: 1.1141 | G Loss: 1.1551\n",
      "Epoch [51/100] | D Loss: 1.1167 | G Loss: 1.0329\n",
      "Epoch [52/100] | D Loss: 1.0134 | G Loss: 1.0638\n",
      "Epoch [53/100] | D Loss: 1.0973 | G Loss: 0.9522\n",
      "Epoch [54/100] | D Loss: 1.2472 | G Loss: 0.8147\n",
      "Epoch [55/100] | D Loss: 1.1398 | G Loss: 0.8296\n",
      "Epoch [56/100] | D Loss: 1.0479 | G Loss: 1.0905\n",
      "Epoch [57/100] | D Loss: 1.2961 | G Loss: 0.7285\n",
      "Epoch [58/100] | D Loss: 1.0033 | G Loss: 1.1797\n",
      "Epoch [59/100] | D Loss: 1.1578 | G Loss: 0.8232\n",
      "Epoch [60/100] | D Loss: 1.1202 | G Loss: 0.9276\n",
      "Epoch [61/100] | D Loss: 1.0082 | G Loss: 1.0067\n",
      "Epoch [62/100] | D Loss: 1.0218 | G Loss: 1.1397\n",
      "Epoch [63/100] | D Loss: 1.1110 | G Loss: 0.8500\n",
      "Epoch [64/100] | D Loss: 1.1572 | G Loss: 0.7760\n",
      "Epoch [65/100] | D Loss: 1.4249 | G Loss: 0.5140\n",
      "Epoch [66/100] | D Loss: 1.0961 | G Loss: 1.1219\n",
      "Epoch [67/100] | D Loss: 1.0212 | G Loss: 1.1525\n",
      "Epoch [68/100] | D Loss: 1.1513 | G Loss: 0.8108\n",
      "Epoch [69/100] | D Loss: 1.0159 | G Loss: 1.1581\n",
      "Epoch [70/100] | D Loss: 1.0030 | G Loss: 1.0473\n",
      "Epoch [71/100] | D Loss: 1.0922 | G Loss: 0.9035\n",
      "Epoch [72/100] | D Loss: 1.2735 | G Loss: 0.7635\n",
      "Epoch [73/100] | D Loss: 1.0334 | G Loss: 1.0221\n",
      "Epoch [74/100] | D Loss: 1.0433 | G Loss: 0.9192\n",
      "Epoch [75/100] | D Loss: 1.1777 | G Loss: 0.9754\n",
      "Epoch [76/100] | D Loss: 1.0256 | G Loss: 1.1102\n",
      "Epoch [77/100] | D Loss: 1.1072 | G Loss: 0.7388\n",
      "Epoch [78/100] | D Loss: 1.1129 | G Loss: 0.9598\n",
      "Epoch [79/100] | D Loss: 0.9393 | G Loss: 1.0250\n",
      "Epoch [80/100] | D Loss: 0.9690 | G Loss: 1.5307\n",
      "Epoch [81/100] | D Loss: 1.2538 | G Loss: 0.9278\n",
      "Epoch [82/100] | D Loss: 1.2850 | G Loss: 0.7083\n",
      "Epoch [83/100] | D Loss: 1.2340 | G Loss: 0.8925\n",
      "Epoch [84/100] | D Loss: 1.2267 | G Loss: 0.5499\n",
      "Epoch [85/100] | D Loss: 1.0784 | G Loss: 1.1531\n",
      "Epoch [86/100] | D Loss: 1.0872 | G Loss: 0.9499\n",
      "Epoch [87/100] | D Loss: 0.9092 | G Loss: 1.2302\n",
      "Epoch [88/100] | D Loss: 1.0207 | G Loss: 1.0898\n",
      "Epoch [89/100] | D Loss: 1.1183 | G Loss: 0.8629\n",
      "Epoch [90/100] | D Loss: 1.0731 | G Loss: 1.0034\n",
      "Epoch [91/100] | D Loss: 1.2154 | G Loss: 1.0602\n",
      "Epoch [92/100] | D Loss: 1.0273 | G Loss: 1.0825\n",
      "Epoch [93/100] | D Loss: 1.0929 | G Loss: 1.4301\n",
      "Epoch [94/100] | D Loss: 1.1998 | G Loss: 1.0799\n",
      "Epoch [95/100] | D Loss: 1.2043 | G Loss: 0.9834\n",
      "Epoch [96/100] | D Loss: 1.2124 | G Loss: 0.7481\n",
      "Epoch [97/100] | D Loss: 1.0767 | G Loss: 1.2588\n",
      "Epoch [98/100] | D Loss: 1.1597 | G Loss: 0.6738\n",
      "Epoch [99/100] | D Loss: 1.1301 | G Loss: 0.8856\n",
      "Epoch [100/100] | D Loss: 0.9297 | G Loss: 1.0296\n",
      "Models saved as generator.pkl and discriminator.pkl\n",
      "Generated Data:\n",
      "[[3.39449768e+02 1.23381406e-01 3.59361458e+00 1.41323969e-01\n",
      "  2.40093164e+03 3.38968420e+00 1.03653000e+06 3.40009219e+04\n",
      "  1.75381426e+04 8.16349158e-12]\n",
      " [3.44224274e+02 3.05941924e-02 2.65656281e+00 5.66600114e-02\n",
      "  1.78314380e+03 3.75995445e+00 1.03618356e+06 3.37281406e+04\n",
      "  1.16830488e+04 2.39177905e-12]\n",
      " [3.32071655e+02 2.56915474e-13 1.96478203e-01 7.05821067e-16\n",
      "  1.82115466e+03 5.60402775e+00 4.80229102e+03 1.86736262e+00\n",
      "  1.12775831e+01 2.02899662e-17]\n",
      " [3.59022858e+02 1.52373696e-05 1.03079665e+00 1.51583564e+00\n",
      "  3.57331266e-03 2.35598373e+00 1.04364244e+06 9.07734999e-21\n",
      "  5.51585352e+03 6.00345123e-17]\n",
      " [3.37764740e+02 1.60666350e-05 4.75164795e+00 2.85168849e-08\n",
      "  4.30019938e+05 4.31899261e+00 1.07326512e+06 5.49502075e-01\n",
      "  4.28876025e+03 6.97488378e-12]\n",
      " [3.39877899e+02 4.29916084e-02 4.36864996e+00 5.11313323e-03\n",
      "  1.92878105e+04 3.52696753e+00 1.04289319e+06 3.39867266e+04\n",
      "  4.04709297e+04 3.12703544e-11]\n",
      " [3.51057617e+02 5.30351978e-03 2.69638681e+00 1.01196709e+02\n",
      "  1.09081052e-01 3.68596268e+00 1.03243244e+06 4.59771055e-18\n",
      "  3.50690312e+04 4.04226552e-14]\n",
      " [3.39862122e+02 7.44436363e-07 3.90720892e+00 2.38834175e-10\n",
      "  1.10987450e+06 6.71344185e+00 2.32498625e+06 3.43489975e-01\n",
      "  1.41254443e+04 1.09590754e-12]\n",
      " [3.39208344e+02 2.07166329e-13 6.90625887e-03 3.31036548e-15\n",
      "  3.63213398e+04 4.80665255e+00 1.03528975e+06 6.97267900e-14\n",
      "  2.42297014e-04 1.89847562e-17]\n",
      " [3.44996674e+02 4.48775977e-22 2.72143952e-04 3.01076283e-25\n",
      "  5.62423711e+04 4.40045643e+00 1.01742975e+06 7.37162659e-26\n",
      "  1.11313785e-08 4.75265850e-23]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")  \n",
    "features = ['Machine', 'DebugSize', 'MajorImageVersion', 'ExportSize', \n",
    "            'IatVRA', 'NumberOfSections', 'SizeOfStackReserve', \n",
    "            'DllCharacteristics', 'ResourceSize', 'BitcoinAddresses']\n",
    "\n",
    "df = df[features]\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "data = torch.tensor(df_scaled, dtype=torch.float32)\n",
    "\n",
    "# --------------------------\n",
    "# Define Generator\n",
    "# --------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, feature_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, feature_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# --------------------------\n",
    "# Define Discriminator\n",
    "# --------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --------------------------\n",
    "# Initialize Models and Optimizers\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 10\n",
    "feature_dim = len(features)\n",
    "G = Generator(latent_dim, feature_dim).to(device)\n",
    "D = Discriminator(feature_dim).to(device)\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# --------------------------\n",
    "# Train GAN\n",
    "# --------------------------\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for real_data in data_loader:\n",
    "        real_data = real_data.to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        real_loss = criterion(D(real_data), real_labels)\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_data = G(z)\n",
    "        fake_loss = criterion(D(fake_data.detach()), fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        gen_loss = criterion(D(fake_data), real_labels)\n",
    "        gen_loss.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {gen_loss.item():.4f}\")\n",
    "\n",
    "# Save the trained models\n",
    "with open(\"generator.pkl\", \"wb\") as f:\n",
    "    pickle.dump(G, f)\n",
    "with open(\"discriminator.pkl\", \"wb\") as f:\n",
    "    pickle.dump(D, f)\n",
    "\n",
    "print(\"Models saved as generator.pkl and discriminator.pkl\")\n",
    "\n",
    "# Generate and Print Synthetic Data\n",
    "z = torch.randn(10, latent_dim).to(device)\n",
    "generated_data = G(z).cpu().detach().numpy()\n",
    "print(\"Generated Data:\")\n",
    "print(scaler.inverse_transform(generated_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a454dbe1-a0e6-4887-8a1a-0e0b5b766d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] | D Loss: 1.0834 | G Loss: 1.0853\n",
      "Epoch [2/100] | D Loss: 1.2615 | G Loss: 0.7242\n",
      "Epoch [3/100] | D Loss: 1.0132 | G Loss: 1.0315\n",
      "Epoch [4/100] | D Loss: 1.1721 | G Loss: 0.9152\n",
      "Epoch [5/100] | D Loss: 1.0872 | G Loss: 0.9221\n",
      "Epoch [6/100] | D Loss: 1.3243 | G Loss: 0.9060\n",
      "Epoch [7/100] | D Loss: 0.9844 | G Loss: 1.0177\n",
      "Epoch [8/100] | D Loss: 0.8376 | G Loss: 1.0926\n",
      "Epoch [9/100] | D Loss: 0.9388 | G Loss: 1.1214\n",
      "Epoch [10/100] | D Loss: 1.9248 | G Loss: 0.8697\n",
      "Epoch [11/100] | D Loss: 0.8560 | G Loss: 1.0711\n",
      "Epoch [12/100] | D Loss: 1.0466 | G Loss: 0.9560\n",
      "Epoch [13/100] | D Loss: 1.1328 | G Loss: 0.8988\n",
      "Epoch [14/100] | D Loss: 1.1131 | G Loss: 0.9054\n",
      "Epoch [15/100] | D Loss: 1.2714 | G Loss: 0.7616\n",
      "Epoch [16/100] | D Loss: 1.2759 | G Loss: 0.7979\n",
      "Epoch [17/100] | D Loss: 1.1479 | G Loss: 0.8356\n",
      "Epoch [18/100] | D Loss: 1.2984 | G Loss: 0.7895\n",
      "Epoch [19/100] | D Loss: 1.1788 | G Loss: 0.9093\n",
      "Epoch [20/100] | D Loss: 1.2161 | G Loss: 0.8694\n",
      "Epoch [21/100] | D Loss: 1.1059 | G Loss: 0.8661\n",
      "Epoch [22/100] | D Loss: 1.1497 | G Loss: 0.8564\n",
      "Epoch [23/100] | D Loss: 0.9662 | G Loss: 0.9557\n",
      "Epoch [24/100] | D Loss: 1.0280 | G Loss: 1.0735\n",
      "Epoch [25/100] | D Loss: 0.8273 | G Loss: 1.3140\n",
      "Epoch [26/100] | D Loss: 0.7154 | G Loss: 1.1740\n",
      "Epoch [27/100] | D Loss: 0.7154 | G Loss: 1.5554\n",
      "Epoch [28/100] | D Loss: 0.6159 | G Loss: 1.5933\n",
      "Epoch [29/100] | D Loss: 0.6302 | G Loss: 1.9826\n",
      "Epoch [30/100] | D Loss: 0.6205 | G Loss: 1.9149\n",
      "Epoch [31/100] | D Loss: 0.6072 | G Loss: 1.8971\n",
      "Epoch [32/100] | D Loss: 0.6724 | G Loss: 1.6740\n",
      "Epoch [33/100] | D Loss: 0.4931 | G Loss: 2.3197\n",
      "Epoch [34/100] | D Loss: 0.5612 | G Loss: 2.1121\n",
      "Epoch [35/100] | D Loss: 0.4995 | G Loss: 2.4036\n",
      "Epoch [36/100] | D Loss: 0.4364 | G Loss: 2.8581\n",
      "Epoch [37/100] | D Loss: 0.6113 | G Loss: 2.8011\n",
      "Epoch [38/100] | D Loss: 0.4015 | G Loss: 3.1206\n",
      "Epoch [39/100] | D Loss: 0.5262 | G Loss: 1.9108\n",
      "Epoch [40/100] | D Loss: 0.5103 | G Loss: 2.8357\n",
      "Epoch [41/100] | D Loss: 0.3625 | G Loss: 2.7650\n",
      "Epoch [42/100] | D Loss: 0.3259 | G Loss: 2.3257\n",
      "Epoch [43/100] | D Loss: 0.6135 | G Loss: 2.0586\n",
      "Epoch [44/100] | D Loss: 0.7024 | G Loss: 1.8079\n",
      "Epoch [45/100] | D Loss: 0.7193 | G Loss: 2.2719\n",
      "Epoch [46/100] | D Loss: 0.6421 | G Loss: 2.7586\n",
      "Epoch [47/100] | D Loss: 0.4770 | G Loss: 3.1330\n",
      "Epoch [48/100] | D Loss: 0.8774 | G Loss: 2.6750\n",
      "Epoch [49/100] | D Loss: 1.1201 | G Loss: 1.0133\n",
      "Epoch [50/100] | D Loss: 0.7793 | G Loss: 3.3882\n",
      "Epoch [51/100] | D Loss: 0.9316 | G Loss: 1.4541\n",
      "Epoch [52/100] | D Loss: 0.8747 | G Loss: 1.5488\n",
      "Epoch [53/100] | D Loss: 1.0328 | G Loss: 2.2540\n",
      "Epoch [54/100] | D Loss: 0.7109 | G Loss: 1.8222\n",
      "Epoch [55/100] | D Loss: 0.8452 | G Loss: 3.4204\n",
      "Epoch [56/100] | D Loss: 1.1222 | G Loss: 1.2516\n",
      "Epoch [57/100] | D Loss: 1.0633 | G Loss: 1.8801\n",
      "Epoch [58/100] | D Loss: 0.9032 | G Loss: 1.1679\n",
      "Epoch [59/100] | D Loss: 1.0095 | G Loss: 2.2373\n",
      "Epoch [60/100] | D Loss: 0.9675 | G Loss: 0.8411\n",
      "Epoch [61/100] | D Loss: 0.8847 | G Loss: 2.0030\n",
      "Epoch [62/100] | D Loss: 0.9395 | G Loss: 1.2287\n",
      "Epoch [63/100] | D Loss: 0.9418 | G Loss: 1.3499\n",
      "Epoch [64/100] | D Loss: 0.9843 | G Loss: 1.5651\n",
      "Epoch [65/100] | D Loss: 0.6766 | G Loss: 2.8664\n",
      "Epoch [66/100] | D Loss: 0.8319 | G Loss: 1.0419\n",
      "Epoch [67/100] | D Loss: 0.8825 | G Loss: 1.4539\n",
      "Epoch [68/100] | D Loss: 1.0093 | G Loss: 1.5184\n",
      "Epoch [69/100] | D Loss: 0.9309 | G Loss: 2.7257\n",
      "Epoch [70/100] | D Loss: 0.7459 | G Loss: 2.6243\n",
      "Epoch [71/100] | D Loss: 0.7588 | G Loss: 1.8179\n",
      "Epoch [72/100] | D Loss: 0.7449 | G Loss: 3.4281\n",
      "Epoch [73/100] | D Loss: 0.7989 | G Loss: 2.5894\n",
      "Epoch [74/100] | D Loss: 0.7881 | G Loss: 1.6455\n",
      "Epoch [75/100] | D Loss: 0.9252 | G Loss: 1.9340\n",
      "Epoch [76/100] | D Loss: 0.7765 | G Loss: 2.1424\n",
      "Epoch [77/100] | D Loss: 1.0689 | G Loss: 1.3585\n",
      "Epoch [78/100] | D Loss: 0.8288 | G Loss: 1.4419\n",
      "Epoch [79/100] | D Loss: 0.8524 | G Loss: 1.3254\n",
      "Epoch [80/100] | D Loss: 0.9509 | G Loss: 2.0477\n",
      "Epoch [81/100] | D Loss: 0.9049 | G Loss: 1.6794\n",
      "Epoch [82/100] | D Loss: 0.6773 | G Loss: 1.6564\n",
      "Epoch [83/100] | D Loss: 1.0829 | G Loss: 1.4064\n",
      "Epoch [84/100] | D Loss: 0.8208 | G Loss: 1.6120\n",
      "Epoch [85/100] | D Loss: 0.7865 | G Loss: 1.6041\n",
      "Epoch [86/100] | D Loss: 0.9043 | G Loss: 2.1530\n",
      "Epoch [87/100] | D Loss: 0.7104 | G Loss: 4.0989\n",
      "Epoch [88/100] | D Loss: 0.9173 | G Loss: 1.8456\n",
      "Epoch [89/100] | D Loss: 0.5903 | G Loss: 2.9608\n",
      "Epoch [90/100] | D Loss: 0.4657 | G Loss: 5.2994\n",
      "Epoch [91/100] | D Loss: 0.8702 | G Loss: 2.3305\n",
      "Epoch [92/100] | D Loss: 0.7549 | G Loss: 1.5849\n",
      "Epoch [93/100] | D Loss: 0.9911 | G Loss: 2.5926\n",
      "Epoch [94/100] | D Loss: 0.7291 | G Loss: 2.5150\n",
      "Epoch [95/100] | D Loss: 1.0529 | G Loss: 1.8732\n",
      "Epoch [96/100] | D Loss: 0.9862 | G Loss: 0.9058\n",
      "Epoch [97/100] | D Loss: 0.7566 | G Loss: 2.1347\n",
      "Epoch [98/100] | D Loss: 0.8292 | G Loss: 0.8695\n",
      "Epoch [99/100] | D Loss: 0.6835 | G Loss: 1.5338\n",
      "Epoch [100/100] | D Loss: 0.9536 | G Loss: 1.0762\n",
      "Models saved as generator.pkl and discriminator.pkl\n",
      "\n",
      "Machine Learning Model Evaluation:\n",
      "Accuracy: 0.5294\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      7055\n",
      "           1       0.43      0.26      0.32      5442\n",
      "\n",
      "    accuracy                           0.53     12497\n",
      "   macro avg       0.50      0.50      0.48     12497\n",
      "weighted avg       0.51      0.53      0.50     12497\n",
      "\n",
      "Machine learning model saved as ml_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --------------------------\n",
    "# Load and Prepare the Data\n",
    "# --------------------------\n",
    "df = pd.read_csv(\"data.csv\")  # Ensure the dataset is in the correct path\n",
    "features = ['Machine', 'DebugSize', 'MajorImageVersion', 'ExportSize', \n",
    "            'IatVRA', 'NumberOfSections', 'SizeOfStackReserve', \n",
    "            'DllCharacteristics', 'ResourceSize', 'BitcoinAddresses']\n",
    "target = 'Benign'  # Define the target column\n",
    "\n",
    "df = df[features + [target]]\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df[features])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "data = torch.tensor(df_scaled, dtype=torch.float32)\n",
    "labels = torch.tensor(df[target].values, dtype=torch.float32)\n",
    "\n",
    "# --------------------------\n",
    "# Define Generator\n",
    "# --------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, feature_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, feature_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# --------------------------\n",
    "# Define Discriminator\n",
    "# --------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --------------------------\n",
    "# Initialize Models and Optimizers\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 10\n",
    "feature_dim = len(features)\n",
    "G = Generator(latent_dim, feature_dim).to(device)\n",
    "D = Discriminator(feature_dim).to(device)\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# --------------------------\n",
    "# Train GAN\n",
    "# --------------------------\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for real_data in data_loader:\n",
    "        real_data = real_data.to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        real_loss = criterion(D(real_data), real_labels)\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_data = G(z)\n",
    "        fake_loss = criterion(D(fake_data.detach()), fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        gen_loss = criterion(D(fake_data), real_labels)\n",
    "        gen_loss.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {gen_loss.item():.4f}\")\n",
    "\n",
    "# Save the trained models\n",
    "with open(\"generator.pkl\", \"wb\") as f:\n",
    "    pickle.dump(G, f)\n",
    "with open(\"discriminator.pkl\", \"wb\") as f:\n",
    "    pickle.dump(D, f)\n",
    "\n",
    "print(\"Models saved as generator.pkl and discriminator.pkl\")\n",
    "\n",
    "# --------------------------\n",
    "# Generate Synthetic Data\n",
    "# --------------------------\n",
    "z = torch.randn(len(df), latent_dim).to(device)\n",
    "generated_data = G(z).cpu().detach().numpy()\n",
    "generated_data = scaler.inverse_transform(generated_data)\n",
    "\n",
    "# --------------------------\n",
    "# Train ML Model on Synthetic Data\n",
    "# --------------------------\n",
    "df_generated = pd.DataFrame(generated_data, columns=features)\n",
    "df_generated[target] = df[target].sample(frac=1, random_state=42).values  # Assign real labels\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_generated[features], df_generated[target], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train RandomForest Model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nMachine Learning Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the ML model\n",
    "with open(\"ml_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "print(\"Machine learning model saved as ml_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eaecb4-5b05-4725-9b3d-c62ce12fe095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
